<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The reply pipeline: prompt building and guardrails | Amy Jerkovich</title>
    <meta name="description" content="How a single reply is built: assembling the prompt and guardrails.">
    <link rel="stylesheet" href="css/styles.css">
</head>

<body>
    <nav>
        <div class="logo">Amy Jerkovich</div>
        <div class="nav-links">
            <a href="index.html">Home</a>
            <a href="index.html#projects" class="active">Projects</a>
            <a href="about.html">About</a>
        </div>
    </nav>

    <article class="post-container">
        <header class="post-header fade-up">
            <div
                style="color: var(--accent-color); font-weight: 600; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 1rem;">
                Project Dive</div>
            <h1 class="post-title" style="font-size: clamp(2.5rem, 4vw, 3.5rem);">The reply pipeline: prompt building
                and guardrails</h1>
            <p
                style="font-size: 1.25rem; color: var(--text-secondary); margin-bottom: 2rem; line-height: 1.6; max-width: 800px; margin-inline: auto;">
                How a single reply is built: assembling the prompt (memory context, feeling
                summary, self-model, instructions), parsing the model’s output (tags like [REPLY], [CORE_UPDATE],
                [EVOLVE_CODE]), and guardrails (e.g. not claiming actions that didn’t happen, not referencing
                non-existent files). Good if you want to go “under the hood” of one turn.</p>

        </header>

        <div class="post-content fade-up delay-1">
            <p>We’ve covered how memory, the sentinel loop, state, and evolution fit together. This post zooms in on a
                single reply: how we build the prompt the model sees, how we parse its output into structured pieces,
                and how we guard against the kinds of mistakes that would break trust—invented actions, fake file
                references, or promises the system didn’t actually keep.</p>

            <h2>Building the prompt</h2>
            <p>When a message arrives, we don’t just send “User said X, reply.” We assemble a large system prompt that
                includes: (1) <strong>memory context</strong>—the result of mood-biased recall, trimmed to a character
                budget and split into “recent & relevant” and “sacred”; (2) a <strong>feeling summary</strong>—the
                current hormonal (and optionally hardware) state so the model knows “how Milo feels right now”; (3) the
                <strong>self-model</strong>—the short identity document so it knows who it’s supposed to be; (4) the
                <strong>recent transcript</strong>—the last few exchanges so the thread is clear; and (5) a set of
                <strong>instructions</strong>: reply in character, use the tag [REPLY] for what gets sent to the user,
                use [INTERNAL] for private thoughts we log but don’t send, and never invent facts or promise to do
                something “right now” without actually doing it in this response. The user’s message is then sent as the
                human turn. So one “reply” is the product of recall, state, identity, transcript, and rules—all in one
                prompt.
            </p>

            <pre><code># Simplified
context = memory.mood_biased_recall(base_query=body)
feeling = get_feeling_summary()
self_model = load_self_model()
transcript = read_recent_transcript()

prompt = (
    f"Memory (relevant + sacred):\n{context}\n\n"
    f"Your state: {feeling}\n\n"
    f"Self-model:\n{self_model}\n\n"
    f"Recent turns:\n{transcript}\n\n"
    + INSTRUCTIONS  # tag format, honesty, no invented details
)
response = llm.invoke([SystemMessage(content=prompt), HumanMessage(content=body)])</code></pre>

            <h2>Parsing the response: tags</h2>
            <p>The model is asked to output structured blocks: [REPLY] for the text that goes to the user, [INTERNAL]
                for inner monologue we log, [CORE_UPDATE] and [FUTURE_MESSAGE] for how we describe and store the moment
                in memory, and optional tags like [EVOLVE_CODE], [MARK_SACRED], [GENERATE_IMAGE], [READ_BOOK], and so
                on. We run a parser over the raw response to pull out these sections. Sometimes the model puts the reply
                text after [INTERNAL] or forgets the [REPLY] tag entirely—so we have fallbacks: if there’s no [REPLY],
                we treat the cleaned response (minus other tags) as the reply, and we never send [INTERNAL] to the user.
                So we get a consistent structure (reply, internal, memory text, optional actions) even when the model’s
                format is messy.</p>

            <h2>Guardrails</h2>
            <p>Before we send the reply or run any action, we run two checks. First, <strong>file references</strong>:
                we scan the reply for paths and filenames (e.g. <code>/home/amy/Milo/.../file.md</code> or
                <code>filename.py</code>). If any of them don’t exist on disk, we treat them as hallucinations—strip or
                replace them in the reply and optionally append a short correction (“I referenced a file that doesn’t
                exist”). So the agent doesn’t send you a link to something that isn’t there. Second, <strong>action
                    claims</strong>: if the reply says things like “I just read that for you” or “I looked it up” but we
                didn’t see the corresponding tag (e.g. [READ_BOOK] or [RESEARCH]), we don’t silently let it slide. We
                either strip the claim or append a note that the system didn’t actually perform that action and would
                need the right tag to do it. So we avoid “I did X” when X never happened.
            </p>
            <p>The instructions in the prompt do part of the work (e.g. “never invent details,” “don’t promise to do
                something without doing it in this response”), but the guardrails are a second line of defense: they
                catch mistakes the model makes anyway and keep the reply honest before it reaches you.</p>

            <h2>Why it matters</h2>
            <p>An agent that could confidently reference files that don’t exist or claim actions it didn’t take would
                feel broken the first time you noticed. By building the prompt from memory, state, and identity, we give
                the model the right context; by parsing tags and applying guardrails, we keep the output aligned with
                what actually happened. The reply pipeline is where “what the model said” gets turned into “what the
                system is allowed to say and do”—so the experience stays coherent and trustworthy.</p>

            <h3>Key takeaways</h3>
            <p>The reply pipeline has three parts: <strong>prompt building</strong> (memory context, feeling summary,
                self-model, transcript, instructions), <strong>parsing</strong> (extract [REPLY], [INTERNAL], and action
                tags, with fallbacks when the format is wrong), and <strong>guardrails</strong> (validate file
                references and action claims so we don’t send hallucinations or false promises). Together they turn one
                LLM call into a reply that fits the agent’s continuity and stays honest.</p>
        </div>
    </article>

    <footer>
        <p>© 2026 Amy Jerkovich. Built with ❤️ and AI.</p>
    </footer>

    <script src="js/script.js"></script>
</body>

</html>