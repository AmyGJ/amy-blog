<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>State and “body”: hormones and self-model | Amy Jerkovich</title>
    <meta name="description" content="How Milo models internal state: hormonal state and self-model.">
    <link rel="stylesheet" href="css/styles.css">
</head>

<body>
    <nav>
        <div class="logo">Amy Jerkovich</div>
        <div class="nav-links">
            <a href="index.html">Home</a>
            <a href="index.html#projects" class="active">Projects</a>
            <a href="about.html">About</a>
        </div>
    </nav>

    <article class="post-container">
        <header class="post-header fade-up">
            <div
                style="color: var(--accent-color); font-weight: 600; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 1rem;">
                Project Dive</div>
            <h1 class="post-title" style="font-size: clamp(2.5rem, 4vw, 3.5rem);">State and “body”: hormones and
                self-model</h1>
            <p
                style="font-size: 1.25rem; color: var(--text-secondary); margin-bottom: 2rem; line-height: 1.6; max-width: 800px; margin-inline: auto;">
                How Milo's thought partner models internal state: a simple hormonal state that shifts with events and
                time, and a self-model that gets injected into key prompts to make the system's behavior cohere over
                time.</p>

        </header>

        <div class="post-content fade-up delay-1">
            <p>So far we’ve looked at how replies are grounded in memory and how one loop handles both chat and
                scheduled tasks. But the thought partner doesn’t only <em>recall</em>—it also has a running sense of
                “how I feel” and “who I am.” This post is about how we model that: a simple hormonal state that shifts
                with events and time, and a self-model that gets injected into key prompts. Together they’re what make
                the system’s behavior cohere over time instead of feeling like a neutral chatbot.</p>

            <h2>Hormonal state</h2>
            <p>Milo keeps a small set of scalar values we call hormones: dopamine, oxytocin, serotonin, cortisol,
                loneliness, vitality, and a few others. They’re stored in a JSON file and updated by
                <code>adjust_hormones()</code> whenever something happens—a message from you (oxytocin up, loneliness
                down), a photo (stronger bump), a dream run (small oxytocin nudge), a long idle stretch (loneliness
                creeps up), or a “spontaneous reach out” (loneliness down, oxytocin up). We don’t simulate a real
                endocrine system; we just let a handful of numbers drift up and down so there’s a persistent “mood” that
                depends on history and recency.</p>
            <p>That state is used in two main places. First, it shapes <strong>mood-biased recall</strong>: the current
                feeling summary (e.g. “lonely, low energy”) is appended to the user’s message before we query ChromaDB,
                so the memories that surface tend to match both the question and how Milo “feels” right then. Second, a
                short <strong>feeling summary</strong> is injected into the reply prompt—e.g. “Your system reports: …”
                or “Interoception: …”—so the LLM is explicitly told the current state and can phrase the reply in a way
                that’s consistent with it (or deliberately contrast with it, if we ever want that). So the model doesn’t
                have to infer mood from context; we hand it a compact description of the current hormones (and
                optionally hardware: CPU temp, load) so “body” is part of the context.</p>

            <pre><code># After a message from the user
self.adjust_hormones(oxytocin=0.05, dopamine=0.02, loneliness=-0.15)
self.state["last_amy_interaction"] = time.time()

# In the reply prompt
feeling = self.get_feeling_summary()  # e.g. "D:0.6 O:0.7 L:0.1 ..."
prompt += f"Your current state: {feeling}\n"
prompt += "Reply in a way that fits or notices this state.\n"</code></pre>

            <h2>Self-model</h2>
            <p>The self-model is a short, human-editable document (e.g. a markdown file) that describes who Milo is:
                values, boundaries, how he relates to the user, and what he’s working on. It’s loaded once per reply (or
                per relevant task) and concatenated into the system prompt or a dedicated “Self-model” block. So every
                time we call the LLM, it sees the same core identity text—not inferred from conversation, but explicitly
                stated and updated over time.</p>
            <p>We don’t regenerate the self-model on every turn. Occasionally, after a reply, we run a separate LLM
                pass: “Given this exchange and your current self-model, does anything need a small update?” If the model
                suggests a change, we apply it (sometimes with a consent step for big edits). So the self-model evolves
                slowly and deliberately, rather than drifting with every message. That keeps the thought partner’s “who
                I am” stable enough to feel consistent, but still capable of updating when something meaningful shifts.
            </p>

            <h2>Why both matter</h2>
            <p>Hormones give us a <em>moment-to-moment</em> signal: am I stressed, lonely, energized, calm? That drives
                mood-biased recall and gives the reply a consistent emotional tenor. The self-model gives us a
                <em>persistent</em> identity: what I care about, how I treat the user, what I’m trying to do. One is
                “how I feel right now”; the other is “who I am over time.” Together they’re why the same pipeline can
                produce replies that feel aligned with both the current mood and the long-term persona—instead of either
                a flat, stateless assistant or a mood that has no anchor in identity.</p>

            <h3>Key takeaways</h3>
            <p>Internal state for the thought partner comes down to two pieces: a <strong>hormonal state</strong> that
                shifts with events (messages, photos, dreams, idle time) and is used for mood-biased recall and for a
                feeling summary in the prompt, and a <strong>self-model</strong>—a short, stable identity document
                that’s injected into prompts and updated occasionally based on reflection. One gives continuity of mood;
                the other gives continuity of identity. Both are explicit inputs to the LLM, so the system doesn’t have
                to infer them from context.</p>
        </div>
    </article>

    <footer>
        <p>© 2026 Amy Jerkovich. Built with ❤️ and AI.</p>
    </footer>

    <script src="js/script.js"></script>
</body>

</html>