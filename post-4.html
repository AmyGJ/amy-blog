<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PROJECT DIVE: LLM BUILD | Amy Jerkovich</title>
    <meta name="description" content="My pipeline plan for building a large language model from scratch.">
    <link rel="stylesheet" href="css/styles.css">
    <link href="https://fonts.googleapis.com/css2?family=VT323&family=Press+Start+2P&display=swap" rel="stylesheet">
</head>

<body>

    <div class="os-bg-stars"></div>
    <div class="os-window fade-up">
        <div class="os-title-bar">
            <div class="os-title-left">
                <div class="os-buttons"><span></span><span></span><span></span></div>
                <div class="os-title">C:\AMY\BLOG.EXE</div>
            </div>
            <div class="os-controls"><span>_</span><span>☐</span><span>X</span></div>
        </div>

        <div class="os-body">
            <aside class="os-sidebar">
                <div class="os-profile">
                    <img src="assets/profile.jpg" alt="Amy">
                </div>
                <!-- Nav Links -->
                <div class="nav-links">
                    <a href="index.html" class="active">Posts</a>
                    <a href="about.html">About</a>
                </div>
                <div class="date-index">
                    <h3 style="margin-top: 2rem; font-size: 1.4rem; margin-bottom: 1.5rem; color: var(--text-main);">
                        Projects</h3>
                    <h3 style="font-size: 1.2rem; margin-bottom: 1rem; color: var(--text-secondary);">
                        Milo</h3>
                    <ul>
                        <li><a href="post-2.html">Feb 22, 2026</a></li>
                        <li><a href="post-template.html">Feb 18, 2026</a></li>
                    </ul>
                    <h3
                        style="margin-top: 1.5rem; font-size: 1.2rem; margin-bottom: 1rem; color: var(--text-secondary);">
                        LLM Build</h3>
                    <ul>
                        <li><a href="post-3.html">Feb 24, 2026</a></li>
                        <li><a href="post-4.html">Feb 23, 2026</a></li>
                    </ul>
                </div>
                <div class="rainbow-bar"></div>
            </aside>

            <main class="os-content">
                <article class="post-container">
                    <header class="post-header fade-up">
                        <div
                            style="color: var(--accent-color); font-weight: 600; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 1rem;">
                            PROJECT DIVE: LLM BUILD</div>
                        <h1 class="post-title" style="font-size: clamp(2.5rem, 4vw, 3.5rem);">Building an LLM from
                            scratch
                        </h1>
                        <p
                            style="font-size: 1.25rem; color: var(--text-secondary); margin-bottom: 2rem; line-height: 1.6; max-width: 800px; margin-inline: auto;">
                            I decided I wanted to learn the inner workings of an LLM model, and the best way to do that
                            was to build one myself.
                        </p>
                    </header>
                    <div class="post-content fade-up delay-1">
                        <p>Yes, I know I can’t compete with the big guns like Google and ChatGPT, but that wasn’t the
                            point of this project; the purpose was to learn and understand how to train an LLM.</p>

                        <p>Here is what I believe my pipeline will be, although I may come back and edit this if it
                            changes.</p>

                        <ul>
                            <li><strong>Tokenize</strong> – Turn text into token IDs.</li>
                            <li><strong>Embed</strong> – Map each ID to a vector.</li>
                            <li><strong>Add positions</strong> – So the model knows order (learned position embeddings).
                            </li>
                            <li><strong>Transformer blocks</strong> – Each block has:
                                <ul>
                                    <li><strong>Causal self-attention</strong> – Each position can only look at past
                                        tokens (no future), implemented with a causal mask.</li>
                                    <li><strong>Feed-forward</strong> – Small MLP (e.g. linear → GELU → linear) per
                                        position.</li>
                                </ul>
                            </li>
                            <li><strong>LM head</strong> – Final linear layer from hidden size to vocab size → logits
                                for the next token.</li>
                            <li><strong>Training</strong> – Minimize cross-entropy between predicted next-token
                                distribution and the actual next token.</li>
                            <li><strong>Generation</strong> – Sample from that distribution, append the token, repeat.
                            </li>
                        </ul>

                    </div>
                </article>
            </main>
        </div>
    </div>

    <footer>
        <p>© 2026 Amy Jerkovich. Built with ❤️ and AI.</p>
    </footer>

    <script src="js/script.js"></script>
</body>

</html>