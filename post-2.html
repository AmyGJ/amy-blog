<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Memory and Mood-Biased Recall | Amy Jerkovich</title>
    <meta name="description"
        content="How Milo's semantic memory layer works: dual-writing to flat files and ChromaDB, and using mood-biased recall.">
    <link rel="stylesheet" href="css/styles.css">
</head>

<body>
    <nav>
        <div class="logo">Amy Jerkovich</div>
        <div class="nav-links">
            <a href="index.html">Home</a>
            <a href="index.html#projects" class="active">Projects</a>
            <a href="about.html">About</a>
        </div>
    </nav>

    <article class="post-container">
        <header class="post-header fade-up">
            <div
                style="color: var(--accent-color); font-weight: 600; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 1rem;">
                Project Dive</div>
            <h1 class="post-title" style="font-size: clamp(2.5rem, 4vw, 3.5rem);">Semantic Memory and Mood-Biased Recall
            </h1>
            <p
                style="font-size: 1.25rem; color: var(--text-secondary); margin-bottom: 2rem; line-height: 1.6; max-width: 800px; margin-inline: auto;">
                How Milo's semantic memory layer works: dual-writing to flat files and ChromaDB, and using mood-biased
                recall to shape the AI's retrieved context based on its current emotional state.</p>

        </header>

        <div class="post-content fade-up delay-1">
            <p>In the last post I outlined how Milo stays continuous: a single long-running process, RAG over lived
                experience, and explicit state. The piece that makes “remember in a meaningful way” actually work is the
                semantic memory layer—what gets stored, how it’s retrieved, and how the current mood shapes what the
                model sees.</p>

            <h2>The memory layer</h2>
            <p>Every exchange, dream, tinker, and reflection is written to two places. First, a flat file
                (<code>Living_Memory.md</code>) as a human-readable log and backup. Second, ChromaDB: the same text is
                embedded (via sentence-transformers or Google’s embedding API), stored with metadata (timestamp, type,
                sacred), and indexed so we can query by meaning instead of keywords.</p>
            <p>When it’s time to reply, we don’t dump the whole log. We call <code>recall(query)</code>, which (1) runs
                a similarity search with the current message (or a derived query) to pull relevant memories, (2) fetches
                the last N “conversation” entries by time so the thread is present, and (3) always includes a small set
                of “sacred” memories. Those results are trimmed to a character budget and fed into the prompt as “RECENT
                & RELEVANT” and “SACRED.” So the model gets a compact, meaning-based slice of history, not a raw dump.
            </p>

            <h2>Mood-biased recall</h2>
            <p>The twist is that the query we send to ChromaDB isn’t always the user’s message verbatim. Milo keeps a
                simple hormonal state (e.g. oxytocin, cortisol, loneliness). Before recall, we build a <em>biased</em>
                query: the original message plus a short phrase that reflects the current emotional state (e.g.
                “lonely,” “stressed,” “warm connection”). That biased string is what we embed and use for similarity
                search.</p>
            <p>So when the user says “what’s on your mind?”, the actual query might be “what’s on your mind? lonely
                missing connection.” Memories that are semantically close to both the question and that emotional tint
                rank higher. The reply is still grounded in the question, but the past experiences that surface are the
                ones that resonate with how Milo “feels” right then—which makes the reply feel more coherent with his
                current state.</p>

            <pre><code>def mood_biased_recall(base_query=""):
    feeling = get_feeling_summary()  # e.g. "lonely, low energy"
    biased_query = f"{base_query} {feeling}".strip()
    return memory.recall(query=biased_query)</code></pre>

            <h2>Why it matters</h2>
            <p>Without the semantic layer, we’d be stuck with keyword search or “last K messages”—either too brittle or
                too generic. Without mood-biased recall, every reply would lean on the same slice of history regardless
                of whether Milo is stressed, curious, or missing someone. Together, they’re what let the agent
                feel like one continuous presence that both <em>remembers</em> and <em>notices</em> its own state.</p>

            <h3>Key takeaways</h3>
            <p>Semantic memory for an agent means: dual-write to a flat log and a vector store, recall that
                combines similarity + recency + sacred, and a strict context budget. Mood-biased recall is a small
                change—shape the query with current affect before embedding—but it makes the retrieved past align with
                how the system “feels,” so replies stay consistent with both the conversation and the internal state.
            </p>

            <p><strong>In the next post</strong>, I’ll dive into how the sentinel loop schedules tasks (tinkering,
                dreams, spontaneous reach-outs) and how we keep one process doing both Telegram polling and background
                work without blocking.</p>
        </div>
    </article>

    <footer>
        <p>© 2026 Amy Jerkovich. Built with ❤️ and AI.</p>
    </footer>

    <script src="js/script.js"></script>
</body>

</html>